# Week 1

### Project specification
This week we had our first meeting with our supervisor and the PHD student who is strongly tied to the project. The choice of what exactly we want to do during the project has been left mostly up to us, given that it relates to the sign language teaching software being developed by Han Phan(the PHD student).

Before I go into the options we have come up with, I'll give some context for them and explain what the project is about. Han has over the last couple years developed a program that records the user making a 'sign' in sign language, and given feedback to the user on how they can improve their sign language. Han's research has been focused mainly on determining what forms of feedback are most useful to the user.

So, given this, there are a couple routes we can take our research.
  * Incorporating virtual reality support into the program. Currently the software displays the feedback only on a screen, with options for different camera angles. Virtual reality would allow the user to freely examine the feedback returned by the program, possibly enhancing the learning experience.
  * Incorporating the virtual reality handsets into the program. Han's software uses the kinect to read the sign made by the user. Now, if you've used the kinect before, you're probably thinking that this results in extremely unpredictable behaviour. You'd be right. Although it's not as inaccurate as it could be, it's definitely approaching the limits of the kinect in its usability. Given this, using the virtual reality handsets could be a huge improvement. There are of course downsides and upsides to this, which I'll go into later if we end up choosing this path.
  * Implementing a scoring system. It would be cool to implement a scoring system for the user that gives them a rating based on how well they make their signs. This would link closely in with Han's work, and it would be interesting to see how much of an effect such a system has on the effectiveness of the system.
  * Write an algorithm to read the movements of the user and determine what sign they are making. Han has currently implemented software to inform the user how close their sign is to any particular sign, and it is easy to see how this could be used to read the movements of the user and determining what sign they are trying to make.
Of course it is also possible that we could do more than one of these, or any particular combination of them.

### Unity
Neither Aretha or I have any experience with Unity, so we also spent a bunch of time getting familiar with it. Hopefully we'll be able to pick it up quite fast.
